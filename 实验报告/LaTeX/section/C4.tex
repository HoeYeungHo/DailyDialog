\section{总结}\label{sec:4}

本实验探讨了预训练编码模型BERT在对话情感识别任务上的应用和效果。实验使用了DailyDialog数据集，该数据集包含了多样的情感和语境，适合用于对话情感识别的研究。实验分别使用了不同的预训练编码模型、循环神经网络结构、多任务损失函数和分类器结构，对模型的性能进行了对比和分析。实验结果表明，预训练编码模型BERT具有强大的表达能力，能够在较少的训练中达到很好的性能，且不需要使用循环神经网络融合上下文信息。实验还发现，模型的规模、复杂度和迁移学习策略对模型的泛化能力和训练速度有重要影响，需要根据任务的规模和复杂程度进行适当的选择和调整。本实验为使用预训练编码模型BERT进行对话情感识别提供了一些有益的启示和经验。



