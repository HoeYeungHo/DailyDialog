\section{模型简介}\label{sec:2}

\subsection{循环神经网络}

前馈神经网络（Feedforward Neural Network，FNN）可视为多个简单非线性函数的复合，其构成了一个复杂的函数。其特征在于每次的输入独立存在，即网络的输出仅取决于当前输入。然而，在许多实际任务中，网络的输出不仅仅与当前输入相关，还与过去一段时间的输入相关，典型案例包括自然语言处理。此外，前馈神经网络要求输入和输出的维度固定，而针对时序数据（如视频、语言、文本等），其长度却是不固定的。正因为上述原因，前馈神经网络在自然语言处理领域显得捉襟见肘。

基于此，循环神经网络（Recurrent Neural Network，RNN）被提出用来解决与时序数据相关的问题。循环神经网络是一类具有短期记忆力的神经网络，其神经元不仅接受其他神经元的信息，还接受自身的信息，形成具有环路的网络结构。

循环神经网络的参数可以通过随时间反向传播（BackPropagation Through Time，BPTT）算法\cite{BPTT}或实时循环学习（Real-Time Recurrent Learning，RTRL）算法\cite{RTRL}进行优化。BPTT算法按照时间的逆序将损失一步步往前传递，当输入序列比较长的时候，会存在长程依赖问题，即梯度消失或梯度爆炸的问题，为了解决这个问题，人们引入门控机制（Gating Mechanism）对循环神经网络进行改进，提出了长短期记忆网络（Long Short-Term Memory Network，LSTM）算法\cite{lstm}和门控循环单元（Gated Recurrent Unit，GRU）算法\cite{GRU}。

在本次实验中，循环神经网络的结构主要应用LSTM算法，但是也会利用RNN算法和GRU算法进行对比，在本小节中，主要对以上这三个算法进行介绍。

\subsubsection{RNN}

\paragraph{简单循环网络}

令向量$\boldsymbol     {x}_t \in \mathbb{R}^M$表示在时刻$t$时网络的输入，$\boldsymbol     {h}_t \in \mathbb{R}^D$表示隐藏层状态（即隐藏层神经元活性值），则$\boldsymbol     {h}_t$不仅和当前时刻的输入$\boldsymbol     {x}_t$相关，也和上一个时刻的隐藏层状态$\boldsymbol     {h}_{t-1}$相关。简单循环网络在时刻$t$的更新公式为：隐藏层净输入$\boldsymbol      z_{t}=\boldsymbol      U\boldsymbol     {h}_{t-1}+\boldsymbol      W\boldsymbol     {x}_{t}+\boldsymbol     {b}$，隐藏层状态$\boldsymbol      h_t=f(\boldsymbol      z_t)=f(\boldsymbol      U\boldsymbol     {h}_{t-1}+\boldsymbol      W\boldsymbol     {x}_{t}+\boldsymbol     {b})$。其中，$\boldsymbol      U \in \mathbb{R}^{D \times D}$为状态－状态权重矩阵，$\boldsymbol     {W} \in \mathbb{R}^{D \times M}$为状态－输入权重矩阵，$\boldsymbol     {b} \in \mathbb{R}^D$为偏置向量，$f(\cdot)$是非线性激活函数，通常为Tanh函数（PyTorch中默认）或Logistic函数。简单循环网络如图\ref{fig:RNN}所示。

\input{fig/rnn}

在自然语言处理领域，有许多不同类型的问题，基于此RNN也产生了许多不同的变体。对于序列数据分类问题，可以使用序列到类别模式；对于序列标注任务，由于输入与输出一一对应，所以可以使用同步的序列到序列模式，即上述的简单循环网络结构；对于输入与输出不是一一对应的任务，如机器翻译等，可以使用异步的序列到序列模式。在此次任务中和PyTorch的实现中，均采用同步序列到序列模式，所以对其他两种模式不作过多的说明。

\paragraph{循环神经网络的技巧}

为了便于后续模型的介绍，在此首先介绍循环神经网络中的Dropout和双向循环神经网络（Bidirectional Recurrent Neural Network，Bi-RNN）。LSTM和GRU的Dropout与双向技巧与此类似。

在RNN的模型中，一般在深度方向（垂直方向）上插入Dropout层，随机设置一些隐藏状态$\boldsymbol h$中的值为0，如图\ref{fig:dropout}所示。

\input{fig/dropout}

在简单循环网络中，当前神经元的隐藏层状态$\boldsymbol {h}_t$与当前时刻的输入$\boldsymbol     {x}_t$相关和上一个时刻的隐藏层状态$\boldsymbol     {h}_{t-1}$相关。但是在序列数据中，某一数据可能不仅与其上文相关，还与其下文相关，Bi-RNN就是用于解决此问题的。在Bi-RNN中有两个RNN分别按照时间顺序和逆序进行展开，最终的输出为两个RNN输出的拼接，如图\ref{fig:Bidirectional}所示。

\input{fig/Bidirectional}

\subsubsection{LSTM网络}

LSTM网络是为了解决长程依赖问题所提出的一个循环神经网络的变体。其引入了新的内部状态和门控机制，使网络能够处理长序列信息并更好地对序列信息进行记忆。

首先利用利用当前时刻的输入$\boldsymbol     {x}_t$相关和上一个时刻的外部状态$\boldsymbol {h}_{t-1}$进行仿射变换可以得到当前时刻的候选状态$\tilde{\boldsymbol {c}}_t$。除此以外，还可以得到三个门，分别是遗忘门、输入门和输出门。其中，遗忘门$\boldsymbol f_t$用于控制上一个时刻的内部状态$\boldsymbol  {c}_{t-1}$需要遗忘多少信息，输入门$\boldsymbol i_t$用于控制当前时刻的候选状态$\tilde{\boldsymbol  {c}}_t$有多少信息需要保存，输出门用于控制当前时刻的内部状态$\boldsymbol  {c}_t$有多少信息需要输出给外部状态$\boldsymbol  {h}_t$。

利用当前时刻的$\tilde{\boldsymbol {c}}_t$和上一时刻的内部状态$\boldsymbol  {c}_{t-1}$分别与遗忘门$\boldsymbol f_t$和输入门$\boldsymbol i_t$点乘可以得到当前时刻的内部状态$\boldsymbol  {c}_{t}$。更进一步，当前时刻的内部状态$\boldsymbol  {c}_{t}$与输出门$\boldsymbol {o}_t$点乘可以得到当前时刻的外部状态$\boldsymbol  {h}_t$。其中，外部状态$\boldsymbol  {h}$每个时刻都会被重写，因此可以看作一种短期记忆，而内部状态$\boldsymbol  {c}$借助遗忘门可以将信息保存一段时间，可以看作是一种长短期记忆。LSTM网络结构图如图\ref{fig:LSTM}所示。

\input{fig/LSTM}

\subsubsection{GRU网络}

GRU网络可以看作是LSTM网络的一种变体，是一种比LSTM网络更加简单的循环神经网络。首先，由于在LSTM网络中，遗忘门$\boldsymbol f$和输入门$\boldsymbol i$是互补关系，具有一定的冗余性。GRU网络直接使用一个更新门$\boldsymbol z$来控制当前状态需要从历史状态$\boldsymbol h$中保留多少信息，以及需要从候选状态$\tilde{\boldsymbol{h}}$中接受多少新信息。此外，与LSTM网络类似，当前时刻的候选状态$\tilde{\boldsymbol{h}}_t$是当前时刻的输入$\boldsymbol     {x}_t$相关和上一个时刻的状态$\boldsymbol {h}_{t-1}$的仿射变换，利用一个重置门$\boldsymbol r_t$来控制当前时刻的候选状态$\tilde{\boldsymbol{h}}_t$是否依赖于上一时刻的状态$\boldsymbol {h}_{t-1}$。GRU网络结构图如图\ref{fig:GRU}所示。

\input{fig/GRU}

\subsection{分词器}

人们对于句子的理解基于对其中每个单词的理解，计算机在理解句子时也不例外，需要逐个理解其中的单词。在自然语言处理中，分词器的任务是将句子拆分为一个个单词。由于这些单词可能不仅仅是完整的词汇单元，还包括一些特殊标记，因此在NLP领域，我们通常将它们称为token，因而分词器也被称为tokenizer。其作用在于将连续的文本序列转换成计算机能够处理的离散单元，为后续的语言模型或其他NLP任务提供输入。这个过程对于计算机理解和处理自然语言的任务至关重要。

根据其工作原理和分割单元的不同，分词器主要可以分为三类，分别是基于词、基于字符以及基于子词。

首先，词是最自然的语言单元，所以一个很自然的想法就是将一个句子分解为一个个单词。对于英语来说，可以利用其天然存在的空格对句子进行拆分（在实际操作中，可能还需要对一些符号进行特殊处理），常用的分词器有spaCy和Moses。虽然中文不具备这样的分隔符，但是类似Jieba、HanLP和LTP等分词器基于规则与模型，可以取得良好的分词效果。虽然基于词的分词器很符合直觉，但是其存在许多问题：

\begin{itemize}
	\item \textbf{词汇表过于庞大：}基于词的分词器的词汇表中可能包含大量出现次数比较少的单词，这样会导致模型大部分参数都集中在输入输出层，不利于模型的学习。
	\item \textbf{OOV（Out-of-Vocabulary）问题：}基于词的分词器无法很好地处理处理未知或罕见的词汇的问题。
	\item \textbf{不利于模型学习词缀之间的关系：}基于词的分词器不利于模型学习词缀之间的关系，例如模型学到的old，older和oldest之间的关系无法泛化到smart、smarter和smartest。
\end{itemize}

针对词汇表过大和OOV问题，可以采用基于字符的方法解决。但是由于字符数量太小，我们在为每个字符学习嵌入向量的时候，每个向量就容纳了太多的语义在内，学习起来非常困难。

所以一种折中的方法诞生了，基于子词的分词器介于上述两者之间，其处理原则是，常用词应该保持原状，生僻词应该拆分成子词以共享token压缩空间。这种方法平衡了词汇量和语义独立性，是相对较优的方案。但是，如何将单词划分为子词又是一个难题，子词的划分方法主要有字节对编码（Byte-Pair Encoding，BPE）、WordPiece、Unigram和SentencePiece。一般来说，Unigram算法与SentencePiece算法一起使用，而在BERT中使用的WordPiece算法，由于WordPiece算法与BPE类似，所以以下内容先介绍BPE算法，然后进一步介绍WordPiece算法。综上所述，分词器的分类如图\ref{fig:token}所示。

\input{fig/分词器}

BPE算法首先从一个包含所有字符的初始词汇表开始，在每次迭代中，BPE算法会统计文本中字符对（或者字节对）的出现频率，然后合并出现频率最高的相邻字符对成为一个新的token。BPE算法会不断迭代这个过程，直到达到预定的词汇表大小或者达到停止条件（如合并次数、词汇表大小等）。合并的过程将不断产生新的子词，形成一个动态变化的词汇表，其中包含单字符、常见组合，以及更长的子词。

WordPiece算法与BPE算法类似，都是基于统计的子词分割算法。但是与BPE算法不同的是，在合并token的过程中，BPE算法选择的是出现频率最高的token对，而WordPiece算法选择的是能够最大化训练集数据似然的token对。具体来说，假设有两个token分别为$\text{token}_i$和$\text{token}_j$，WordPiece算法选择$\begin{aligned} \frac{P(\text{token}_i\text{token}_j)}{P(\text{token}_i)P(\text{token}_j)} \end{aligned} $最大的token对。

\subsection{Transformer架构与预训练编码模型BERT}

\subsubsection{Transformer架构}

Transformer架构由Vaswani等人于2017年提出，其完全抛弃了传统的循环神经网络和卷积神经网络（Convolutional Neural Networks，CNN），转而采用自注意力机制。这意味着它可以并行处理输入序列，避免了RNN的逐步处理，极大地提高了计算效率。

\paragraph{自注意力（Self-Attention）机制与多头注意力机制}

自注意力机制允许模型在处理序列数据时同时考虑序列中各个位置的信息，并赋予不同位置的词语不同的权重，从而更好地捕捉上下文关系。

对于一个输入序列（比如文本句子），首先通过词嵌入或其他方式将每个词或符号转换为一个矩阵表示，记作$\boldsymbol  {X}=\left[\boldsymbol  {x}_1, \cdots, \boldsymbol  {x}_N\right] \in \mathbb{R}^{D_{x} \times N}$。自注意力机制将输入项通过线性映射转化为三个部分，分别为查询矩阵$\boldsymbol{Q}=\boldsymbol{W}_Q \boldsymbol{X} \in \mathbb{R}^{D_k \times N}$、键矩阵$\boldsymbol{K}=\boldsymbol{W}_K \boldsymbol{X} \in \mathbb{R}^{D_k \times N}$和值矩阵$\boldsymbol{V}=\boldsymbol{W}_V \boldsymbol{X} \in \mathbb{R}^{D_v \times N}$。其中$\boldsymbol  {W}_Q \in \mathbb{R}^{D_k \times D_x}$，$\boldsymbol  {W}_K \in \mathbb{R}^{D_k \times D_x}$，$\boldsymbol  {W}_V \in \mathbb{R}^{D_v \times D_x}$分别为可学习的线性映射的参数矩阵。通过计算查询矩阵$\boldsymbol{Q}$与键矩阵$\boldsymbol K$之间的相似度，得到每个位置对于当前查询矩阵$\boldsymbol{Q}$的权重分布。通过对每个位置的权重与对应位置的值矩阵$\boldsymbol{V}$进行加权求和，得到当前位置的自注意力表示。输出矩阵序列可以简写为

$$
\boldsymbol{H}=\boldsymbol{V} \operatorname{softmax}\left(\frac{\boldsymbol{K}^{\mathrm{T}} \boldsymbol{Q}}{\sqrt{D_k}}\right)
$$

整个过程如图\ref{fig:SelfAttention}所示。

\input{fig/SelfAttention}

为了增加模型的表征能力，自注意力通常采用多头机制，将输入序列$\boldsymbol X$利用不同的线性映射得到多组查询矩阵$\boldsymbol{Q}$、键矩阵$\boldsymbol K$和值矩阵$\boldsymbol{V}$并分别计算多组注意力表示，最后进行拼接或加权汇总。

\paragraph{Transformer结构}

Transformer由编码器和解码器组成，其中编码器负责将输入序列转换为隐藏表示，解码器则基于编码器的输出生成目标序列。其中，编码器由多个具有相同结构的编码器模块堆叠而成，每个编码器模块包含多头自注意力模块和带残差连接的前馈神经网络模块，用于将输入序列中的信息转换为一系列表示，捕捉输入序列的特征和上下文信息并作为解码器的输入，为解码器提供序列的表示和上下文信息。解码器由多个解码器模块组成，每个编码器模块包含自注意力模块、编码器-解码器注意力模块和前馈神经网络模块三部分，通过接收来自编码器的表示和上下文信息和逐步解码和自注意力计算，生成符合任务要求的输出序列。Transformer结构如图\ref{fig:Transformer}所示。

\input{fig/Transformer}

在具体实现中，Transformer结构还有一些细节：

\begin{itemize}
	\item \textbf{位置编码：}将位置编码添加到编码器和解码器堆栈底部输入向量中，给予词向量序列信息。位置编码被表示为
	$$
	\begin{aligned}
		P E_{(p o s, 2 i)} & =\sin \left(\frac{\text { pos }}{10000^{2 i / d_{\mathrm{model}}}}\right) \\
		P E_{(p o s, 2 i+1)} & =\cos \left(\frac{\text { pos } }{ 10000^{2 i / d_{\mathrm{model}}}}\right)
	\end{aligned}
	$$
	其中，$\text{pos}$是位置，$i$是维度，$d_\text{model}(=512)$是模型的维度。这样，位置编码的每个维度对应于一条正弦曲线，如图\ref{fig:PositionalEncoding}所示。
	\input{fig/PositionalEncoding}
	\item \textbf{注意力掩码：}在每个多头注意力模块中，即在编码器和解码器的多头自注意力模块以及在解码器的编码器-解码器注意力模块中都需要进行掩码，而且掩码的方法以及作用不尽相同。首先，在编码器和解码器的多头自注意力模块，在输入序列中padding对应的位置，将输出的注意力分数归零，以确保padding对自注意力的计算没有贡献。具体来说，在计算softmax之前，对分子上被屏蔽的元素设置为无穷大，这样softmax就会把这些值变成零且能保证softmax输出之和仍为1。在解码器的编码器-解码器注意力模块中，掩码的作用是防止解码器在当前时间步预测时可以获取未来时间步的信息。
	\item \textbf{层归一化：}在序列任务特别是NLP任务中，序列数据的长度可能不一样，这会导致批量的均值和方差估计不准确，所以在Transformer结构中使用层归一化代替批量归一化。
\end{itemize}

\subsubsection{预训练编码模型BERT}

BERT由Google在2018年提出，是一种基于Transformer架构的预训练语言模型。它在自然语言处理领域取得了巨大的成功，通过无监督的方式在大规模文本语料上进行预训练，然后在各种NLP任务上进行微调，取得了令人瞩目的效果。在本小节中，主要对BERT的结构以及预训练和微调的方法进行阐述。

\paragraph{BERT的结构}

BERT采用了与Transformer的编码器类似的结构，由嵌入层（Embedding Layer）、多个具有相同结构的编码器模块以及一个全连接层组成，具体结构如图\ref{fig:BERT}所示。$\text{BERT}_\text{BASE}$和$\text{BERT}_\text{LARGE}$的结构基本相同，只是$\text{BERT}_\text{LARGE}$模型的广度和深度均要比$\text{BERT}_\text{BASE}$大。由于下述实验主要使用预训练编码模型$\text{BERT}_\text{BASE}$，所以在此只对其进行说明。

\input{fig/BERT}

首先是嵌入层。嵌入层是NLP模型的关键组成部分。尽管分词器已将句子转化为一系列token，但这些token原始形式并不能直接供神经网络使用。嵌入层的任务是将这些token转换为密集的向量表示。在BERT中，输入由三部分组成：token、句子编码和位置编码。句子编码用于区分词向量来自不同句子，而位置编码则帮助模型理解词语在句子中的位置关系。

对于BERT的三个输入部分，分别经过嵌入层转换为Token\_Embeddings、Segment\_Embeddings和Position\_Embeddings\footnote{在HuggingFace的实现中，三个嵌入层分别命名为word\_embeddings、token\_type\_embeddings和position\_embeddings。}。这些部分的向量表示会相加，然后通过一个层归一化和Dropout层，生成最终的输出向量。这样的处理方式使得模型能够综合考虑词语的语义、句子边界以及词语在句子中的位置信息，为模型提供了更为丰富和全面的输入。整个嵌入层的工作原理如图\ref{fig:BERTinput}所示。

\input{fig/BERT输入}

接下来是多个结构相似的编码器模块。BERT的编码器模块与Transformer的编码器模块类似，不同之处在于在每个编码器模块之后，BERT添加了中间层和输出层，中间层通过全连接层将维度扩展，而输出层再将维度缩减回原始大小。编码器模块如图\ref{fig:BERTEncoder}所示。

\input{fig/BERTEncoder}

这个过程在模型中引入了额外的非线性转换，中间层的维度扩展使得模型能够对特征进行更深入的变换和学习，从而捕捉更复杂、更抽象的特征。而在输出层，维度的缩减则有助于保留重要的信息，并确保模型的输出与预期的维度相匹配。这种操作方式有助于增强模型的表征能力，提高其对不同语境和语义信息的理解。

编码器模块的输出经过一个全连接层得到最终的输出。对于BERT而言，其输入与输出的维度是相同的，其输出是对输入信息的一种编码，这种编码信息经过不同的神经网络可以适用于不同的下游任务。

\paragraph{BERT的预训练}

预训练的主要目标是使模型能够学到通用的语言知识。相对于有监督训练，无监督训练的成本更低，更适用于预训练任务。BERT在预训练过程中将两个句子拼接在一起，并对其中一些token进行随机遮挡。在这个预训练过程中，BERT模型完成两个子任务，即下一个句子预测（Next Sentence Prediction，NSP）和掩蔽语言模型（Masked Language Model，MLM）。

具体而言，NSP任务旨在预测输入的两个句子在原始文本中是否是连续的，即它们是否是相邻的句子。这个任务有助于模型理解文本之间的逻辑关系和连贯性。而在MLM任务中，输入句子中的一定比例（15\%）的token会被随机遮挡或替换为特殊的标记。模型的目标是根据上下文中其他token的信息来预测被遮挡的token的原始内容。BERT预训练过程如图\ref{fig:BERTPretraining}所示。

\input{fig/BERTPretraining}

此外，为了使训练阶段和微调阶段输入的序列信息尽量相同，对于15\%的遮挡标记，80\%的位置会使用[MASK]代替，10\%的单词替换为一个随机的单词，而另外的10\%的单词则保持不变。这种预训练的设计使得BERT模型能够在学习通用语言表示的同时，适应各种自然语言处理任务。

\paragraph{BERT的微调}

为了适应各种下游任务，经过预训练的BERT模型通过额外简单的分类器进行有监督训练，以将其输出应用于不同领域。微调后的BERT模型展现出了优秀的泛化性能。

BERT的微调主要针对四类下游任务：句子对分类、单句分类、问答和单句标注。针对不同任务，需要利用预训练BERT模型的不同输出部分。具体来说，对于句子对分类和单句分类，使用了特殊的[CLS]标记对应的输出，并将其输入到分类器中进行分类，输出结果即为所需分类；至于问答任务，则利用第二个句子中所有标记的输出进行处理，以定位正确答案在第二个句子中的位置；对于单句标注任务，使用了除[CLS]标记外其他所有标记对应的输出进行分类。这种针对性的输出选择策略有助于最大化BERT模型在各个任务上的效能。BERT微调过程如图\ref{fig:BERTFineTunning}所示。

\input{fig/BERTFineTunning}








































































































